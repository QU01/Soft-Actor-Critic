{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-jzK_95qBGN3",
    "outputId": "7a88c4c3-cd93-4ccc-f297-fd1ae4bd30f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "software-properties-common is already the newest version (0.99.9.10).\n",
      "The following additional packages will be installed:\n",
      "  libgles-dev libgles1 libglew2.1 libglvnd-dev libopengl-dev libosmesa6\n",
      "Suggested packages:\n",
      "  glew-utils\n",
      "The following NEW packages will be installed:\n",
      "  libgl1-mesa-dev libgl1-mesa-glx libgles-dev libgles1 libglew-dev libglew2.1\n",
      "  libglvnd-dev libopengl-dev libosmesa6 libosmesa6-dev\n",
      "0 upgraded, 10 newly installed, 0 to remove and 27 not upgraded.\n",
      "Need to get 3,438 kB of archives.\n",
      "After this operation, 17.1 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgl1-mesa-glx amd64 21.2.6-0ubuntu0.1~20.04.2 [5,536 B]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgles1 amd64 1.3.2-1~ubuntu0.20.04.2 [10.3 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgles-dev amd64 1.3.2-1~ubuntu0.20.04.2 [47.9 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libopengl-dev amd64 1.3.2-1~ubuntu0.20.04.2 [3,584 B]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libglvnd-dev amd64 1.3.2-1~ubuntu0.20.04.2 [11.6 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgl1-mesa-dev amd64 21.2.6-0ubuntu0.1~20.04.2 [6,420 B]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/universe amd64 libglew2.1 amd64 2.1.0-4 [155 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 libglew-dev amd64 2.1.0-4 [134 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libosmesa6 amd64 21.2.6-0ubuntu0.1~20.04.2 [3,054 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libosmesa6-dev amd64 21.2.6-0ubuntu0.1~20.04.2 [8,844 B]\n",
      "Fetched 3,438 kB in 2s (1,597 kB/s)\n",
      "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
      "(Reading database ... 129499 files and directories currently installed.)\n",
      "Preparing to unpack .../0-libgl1-mesa-glx_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\n",
      "Unpacking libgl1-mesa-glx:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n",
      "Selecting previously unselected package libgles1:amd64.\n",
      "Preparing to unpack .../1-libgles1_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\n",
      "Unpacking libgles1:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\n",
      "Selecting previously unselected package libgles-dev:amd64.\n",
      "Preparing to unpack .../2-libgles-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\n",
      "Unpacking libgles-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\n",
      "Selecting previously unselected package libopengl-dev:amd64.\n",
      "Preparing to unpack .../3-libopengl-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\n",
      "Unpacking libopengl-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\n",
      "Selecting previously unselected package libglvnd-dev:amd64.\n",
      "Preparing to unpack .../4-libglvnd-dev_1.3.2-1~ubuntu0.20.04.2_amd64.deb ...\n",
      "Unpacking libglvnd-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\n",
      "Selecting previously unselected package libgl1-mesa-dev:amd64.\n",
      "Preparing to unpack .../5-libgl1-mesa-dev_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\n",
      "Unpacking libgl1-mesa-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n",
      "Selecting previously unselected package libglew2.1:amd64.\n",
      "Preparing to unpack .../6-libglew2.1_2.1.0-4_amd64.deb ...\n",
      "Unpacking libglew2.1:amd64 (2.1.0-4) ...\n",
      "Selecting previously unselected package libglew-dev:amd64.\n",
      "Preparing to unpack .../7-libglew-dev_2.1.0-4_amd64.deb ...\n",
      "Unpacking libglew-dev:amd64 (2.1.0-4) ...\n",
      "Selecting previously unselected package libosmesa6:amd64.\n",
      "Preparing to unpack .../8-libosmesa6_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\n",
      "Unpacking libosmesa6:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n",
      "Selecting previously unselected package libosmesa6-dev:amd64.\n",
      "Preparing to unpack .../9-libosmesa6-dev_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\n",
      "Unpacking libosmesa6-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n",
      "Setting up libgles1:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\n",
      "Setting up libgl1-mesa-glx:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n",
      "Setting up libglew2.1:amd64 (2.1.0-4) ...\n",
      "Setting up libopengl-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\n",
      "Setting up libgles-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\n",
      "Setting up libosmesa6:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n",
      "Setting up libglvnd-dev:amd64 (1.3.2-1~ubuntu0.20.04.2) ...\n",
      "Setting up libglew-dev:amd64 (2.1.0-4) ...\n",
      "Setting up libosmesa6-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n",
      "Setting up libgl1-mesa-dev:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  patchelf\n",
      "0 upgraded, 1 newly installed, 0 to remove and 27 not upgraded.\n",
      "Need to get 53.4 kB of archives.\n",
      "After this operation, 153 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 patchelf amd64 0.10-2build1 [53.4 kB]\n",
      "Fetched 53.4 kB in 1s (71.4 kB/s)\n",
      "Selecting previously unselected package patchelf.\n",
      "(Reading database ... 129593 files and directories currently installed.)\n",
      "Preparing to unpack .../patchelf_0.10-2build1_amd64.deb ...\n",
      "Unpacking patchelf (0.10-2build1) ...\n",
      "Setting up patchelf (0.10-2build1) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y \\\n",
    "    libgl1-mesa-dev \\\n",
    "    libgl1-mesa-glx \\\n",
    "    libglew-dev \\\n",
    "    libosmesa6-dev \\\n",
    "    software-properties-common\n",
    "\n",
    "!apt-get install -y patchelf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gcP-7wDGBHaS",
    "outputId": "dc23497b-0c0b-410d-b17a-1878285e7d80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym) (6.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.21.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym) (3.11.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting free-mujoco-py\n",
      "  Using cached free_mujoco_py-2.1.6-py3-none-any.whl (14.1 MB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.8/dist-packages (from free-mujoco-py) (1.21.6)\n",
      "Collecting fasteners==0.15\n",
      "  Using cached fasteners-0.15-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from free-mujoco-py) (1.15.1)\n",
      "Requirement already satisfied: Cython<0.30.0,>=0.29.24 in /usr/local/lib/python3.8/dist-packages (from free-mujoco-py) (0.29.33)\n",
      "Collecting glfw<2.0.0,>=1.4.0\n",
      "  Using cached glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n",
      "Requirement already satisfied: imageio<3.0.0,>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from free-mujoco-py) (2.9.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from fasteners==0.15->free-mujoco-py) (1.15.0)\n",
      "Collecting monotonic>=0.1\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.21)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from imageio<3.0.0,>=2.9.0->free-mujoco-py) (7.1.2)\n",
      "Installing collected packages: monotonic, glfw, fasteners, free-mujoco-py\n",
      "Successfully installed fasteners-0.15 free-mujoco-py-2.1.6 glfw-1.12.0 monotonic-1.6\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install free-mujoco-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "Mh7m61VvXFWT",
    "outputId": "b6b9d8b8-936c-4a69-b50b-f6fa96c16c41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: gym[mujoco] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[mujoco]) (2.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[mujoco]) (6.0.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[mujoco]) (1.21.6)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[mujoco]) (0.0.8)\n",
      "Collecting imageio>=2.14.1\n",
      "  Downloading imageio-2.25.0-py3-none-any.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mujoco==2.2.0\n",
      "  Downloading mujoco-2.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyopengl in /usr/local/lib/python3.8/dist-packages (from mujoco==2.2.0->gym[mujoco]) (3.1.6)\n",
      "Requirement already satisfied: glfw in /usr/local/lib/python3.8/dist-packages (from mujoco==2.2.0->gym[mujoco]) (1.12.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from mujoco==2.2.0->gym[mujoco]) (1.3.0)\n",
      "Collecting pillow>=8.3.2\n",
      "  Downloading Pillow-9.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[mujoco]) (3.11.0)\n",
      "Installing collected packages: pillow, mujoco, imageio\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 7.1.2\n",
      "    Uninstalling Pillow-7.1.2:\n",
      "      Successfully uninstalled Pillow-7.1.2\n",
      "  Attempting uninstall: imageio\n",
      "    Found existing installation: imageio 2.9.0\n",
      "    Uninstalling imageio-2.9.0:\n",
      "      Successfully uninstalled imageio-2.9.0\n",
      "Successfully installed imageio-2.25.0 mujoco-2.2.0 pillow-9.4.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "PIL"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install gym[mujoco]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KBBLN34E96P1"
   },
   "outputs": [],
   "source": [
    "import mujoco_py\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6N_Ps1id97Ng"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"HalfCheetah-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OPE62e-gCGx9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import * \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uXNFHbE2Cjxl",
    "outputId": "78b132f1-be3b-43e4-b732-90936b783ee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17,)\n",
      "[1. 1. 1. 1. 1. 1.]\n",
      "[-1. -1. -1. -1. -1. -1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(env.reset().shape)\n",
    "print(env.action_space.high), print(env.action_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hX5hmjIlE7hF",
    "outputId": "91d04af5-1047-4811-fc2a-e9ca0ca97dc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lxlVCGESDViZ",
    "outputId": "2107ebec-1d24-47ca-9a76-420d3b01eb41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.04984378, -0.12455043,  0.25254534,  0.27222136,  0.35601046,\n",
       "         0.27977517,  0.21869659,  0.18465768,  1.45686154, -1.13167554,\n",
       "        -3.0367347 ,  8.70556711,  8.89775993,  7.45081049,  9.55547678,\n",
       "         7.80910497,  7.47987164]),\n",
       " 0.4710522792822871,\n",
       " False,\n",
       " {'reward_run': 1.0710522792822872, 'reward_ctrl': -0.6000000000000001})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step([1., 1., 1., 1., 1., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-a49WvU1C-EQ"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, max_size, batch_size):\n",
    "        \n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def append(self, state, next_state, action, reward, done, info):\n",
    "        \n",
    "        sample = (state, next_state, action, reward, done, info)\n",
    "        \n",
    "        if len(self.buffer) >= self.max_size:\n",
    "            \n",
    "            self.buffer.pop(0)\n",
    "        \n",
    "        self.buffer.append(sample)\n",
    "        \n",
    "    def sample(self):\n",
    "        \n",
    "        idxs = np.random.choice(np.arange(len(self.buffer)), size = self.batch_size)\n",
    "        \n",
    "        states = np.array([self.buffer[i][0] for i in idxs])\n",
    "        next_states = np.array([self.buffer[i][1] for i in idxs])\n",
    "        actions = np.array([self.buffer[i][2] for i in idxs])\n",
    "        rewards = np.array([self.buffer[i][3] for i in idxs])\n",
    "        dones = np.array([self.buffer[i][4] for i in idxs])\n",
    "            \n",
    "        return states, next_states, actions, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "8cQVVxgYEQBt"
   },
   "outputs": [],
   "source": [
    "class ValueNetwork(tf.keras.Model):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = layers.Dense(hidden_dim, activation='relu', input_shape=(None, state_dim))\n",
    "        self.linear2 = layers.Dense(hidden_dim, activation='relu')\n",
    "        self.linear3 = layers.Dense(1)\n",
    "\n",
    "        \n",
    "    def call(self, state):\n",
    "        x = self.linear1(state)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9RgtM5e0EdOu"
   },
   "outputs": [],
   "source": [
    "class SoftCritic(tf.keras.Model):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size):\n",
    "        super(SoftCritic, self).__init__()\n",
    "        \n",
    "        self.linear1 = tf.keras.layers.Dense(hidden_size, input_shape=(None, num_inputs + num_actions), activation='relu')\n",
    "        self.linear2 = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "        self.linear3 = tf.keras.layers.Dense(1)\n",
    "        \n",
    "        \n",
    "    def call(self, state, action):\n",
    "        x = tf.concat([state, action], axis=1)\n",
    "        x = tf.keras.layers.ReLU()(self.linear1(x))\n",
    "        x = tf.keras.layers.ReLU()(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "5XK2mL6oFTEv"
   },
   "outputs": [],
   "source": [
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, state_dim, num_actions, hidden_size, action_high=-1, action_low=1):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.action_high = action_high\n",
    "        self.action_low = action_low\n",
    "        \n",
    "        self.linear1 = layers.Dense(hidden_size, activation='relu', input_shape=(None, state_dim))\n",
    "        self.linear2 = layers.Dense(hidden_size, activation='relu')\n",
    "        \n",
    "        self.mean_linear = layers.Dense(num_actions)\n",
    "        self.log_std_linear = layers.Dense(num_actions)\n",
    "        \n",
    "    def call(self, state):\n",
    "        x = self.linear1(state)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        mean    = self.mean_linear(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = tf.clip_by_value(log_std, self.action_low, self.action_high)\n",
    "        \n",
    "        return mean, log_std\n",
    "    \n",
    "    def evaluate(self, state, epsilon=1e-6):\n",
    "        mean, log_std = self.call(state)\n",
    "        std = tf.exp(log_std)\n",
    "        \n",
    "        normal = tfp.distributions.Normal(mean, std)\n",
    "        z = normal.sample()\n",
    "        action = tf.tanh(z)\n",
    "        \n",
    "        log_prob = normal.log_prob(z) - tf.math.log(1 - tf.pow(action, 2) + epsilon)\n",
    "        log_prob = tf.reduce_sum(log_prob, axis=-1, keepdims=True)\n",
    "        \n",
    "        return action, log_prob, z, mean, log_std\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        mean, log_std = self.call(tf.expand_dims(state, axis=0))\n",
    "        std = tf.exp(log_std)\n",
    "        \n",
    "        normal = tfp.distributions.Normal(mean, std)\n",
    "        z      = normal.sample()\n",
    "        action = tf.tanh(z)\n",
    "        \n",
    "        return action.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "HLXEn7HBOQYc"
   },
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    \n",
    "    def __init__(self, env, value_net, value_target, dqn, dqn2, policy_net, replay_buffer, gamma, learning_rate):\n",
    "        \n",
    "        self.env = env\n",
    "        self.dqn = dqn\n",
    "        self.dqn2 = dqn2\n",
    "        self.value_net = value_net\n",
    "        self.value_target = value_target\n",
    "        self.policy = policy_net\n",
    "        self.experience_replay = replay_buffer\n",
    "        self.counter = 0\n",
    "        self.update_rate = 80\n",
    "        self.gamma = gamma\n",
    "        self.value_net_opt = optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.critic_1_opt = optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.critic_2_opt = optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.policy_opt = optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "\n",
    "      if len(self.experience_replay.buffer) < self.experience_replay.batch_size:\n",
    "\n",
    "        return\n",
    "\n",
    "      states, next_states, actions, rewards, dones = self.experience_replay.sample()\n",
    "\n",
    "      #Training Critic\n",
    "      with tf.GradientTape() as tape:\n",
    "        with tf.GradientTape() as tape2:\n",
    "          q_values1 = self.dqn(states, actions)\n",
    "          q_values2 = self.dqn2(states, actions)\n",
    "          target_value = self.value_target(states)\n",
    "\n",
    "          target_q_value = rewards.reshape(-1,1) + (1-dones.reshape(-1,1))*self.gamma*target_value\n",
    "\n",
    "          loss1 = tf.reduce_mean(tf.square(q_values1 - target_q_value))\n",
    "          loss2 = tf.reduce_mean(tf.square(q_values2 - target_q_value))\n",
    "\n",
    "      # Obtener los gradientes\n",
    "      grads1 = tape.gradient(loss1, self.dqn.trainable_variables)\n",
    "      grads2 = tape2.gradient(loss2, self.dqn2.trainable_variables)\n",
    "\n",
    "      # Actualizar los parámetros\n",
    "      self.critic_1_opt.apply_gradients(zip(grads1, self.dqn.trainable_variables))\n",
    "      self.critic_2_opt.apply_gradients(zip(grads2, self.dqn2.trainable_variables))\n",
    "\n",
    "      #Training Value Net\n",
    "\n",
    "      new_actions, log_probs, z, mean, log_std = self.policy.evaluate(states)\n",
    "\n",
    "      with tf.GradientTape() as tape:\n",
    "\n",
    "        prior_values = self.value_net(states)\n",
    "        q_values1 = self.dqn(states, new_actions)\n",
    "        q_values2 = self.dqn2(states, new_actions)\n",
    "        if tf.math.reduce_mean(q_values1) >= tf.math.reduce_mean(q_values2):\n",
    "          q_value = q_values1 \n",
    "        else:\n",
    "          q_value = q_values2\n",
    "\n",
    "        target_values = q_value - log_probs\n",
    "\n",
    "        value_loss = tf.math.reduce_mean(tf.square(prior_values-target_values))\n",
    "      \n",
    "      grads_value = tape.gradient(value_loss, self.value_net.trainable_variables)\n",
    "\n",
    "      self.value_net_opt.apply_gradients(zip(grads_value,self.value_net.trainable_variables))\n",
    "\n",
    "      #Training Policy\n",
    "\n",
    "      with tf.GradientTape() as tape:\n",
    "\n",
    "        new_actions, log_probs, z, mean, log_std = self.policy.evaluate(states)\n",
    "\n",
    "        q_values1 = self.dqn(states, new_actions)\n",
    "        q_values2 = self.dqn2(states, new_actions)\n",
    "        if tf.math.reduce_mean(q_values1) >= tf.math.reduce_mean(q_values2):\n",
    "          q_value = q_values1 \n",
    "        else:\n",
    "          q_value = q_values2\n",
    "        policy_loss = tf.reduce_mean(log_probs - q_value)\n",
    "\n",
    "      grads_policy = tape.gradient(policy_loss, self.policy.trainable_variables)\n",
    "      self.policy_opt.apply_gradients(zip(grads_policy, self.policy.trainable_variables))\n",
    "\n",
    "      self.value_target.set_weights(self.value_net.get_weights())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "w9qsdsztHFzN"
   },
   "outputs": [],
   "source": [
    "value_net = ValueNetwork(17, 32)\n",
    "value_target = ValueNetwork(17, 32)\n",
    "value_target.set_weights(value_net.get_weights())\n",
    "\n",
    "critic_1 = SoftCritic(17, 6, 32)\n",
    "critic_2 = SoftCritic(17, 6, 32)\n",
    "\n",
    "actor = Actor(17, 6, 32)\n",
    "\n",
    "experience_replay = ReplayBuffer(100000, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "VqxWcm9sIrcF"
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "episodes = 200 \n",
    "learning_rate = 3e-4\n",
    "\n",
    "\n",
    "value_net_opt = optimizers.Adam(learning_rate=learning_rate)\n",
    "critic_1_opt =  optimizers.Adam(learning_rate=learning_rate)\n",
    "critic_2_opt =  optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "policy_opt = optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "iUvyVP3hQq4s"
   },
   "outputs": [],
   "source": [
    "sac = SAC(env, value_net, value_target, critic_1, critic_2, actor, experience_replay, gamma, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "9CFFjFnlMVD9"
   },
   "outputs": [],
   "source": [
    "returns = []\n",
    "avg_returns = []\n",
    "total_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4u_xYXzfMfWO",
    "outputId": "d0d109cc-dd00-4fe9-f449-dce066f8b826"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0/200 return of -544.08041437002 average reward of: -544.08041437002in 1000 steps\n",
      "Episode: 1/200 return of -388.20363917574366 average reward of: -466.1420267728818in 1000 steps\n",
      "Episode: 2/200 return of -409.32331225746003 average reward of: -447.20245526774124in 1000 steps\n",
      "Episode: 3/200 return of -414.97393003577827 average reward of: -439.1453239597505in 1000 steps\n",
      "Episode: 4/200 return of -248.06427721751615 average reward of: -400.9291146113036in 1000 steps\n",
      "Episode: 5/200 return of -272.0694809060938 average reward of: -379.4525089937686in 1000 steps\n",
      "Episode: 6/200 return of -305.5794577183668 average reward of: -368.89921595442553in 1000 steps\n",
      "Episode: 7/200 return of -457.70948336774114 average reward of: -380.00049938109in 1000 steps\n",
      "Episode: 8/200 return of -439.8349453607148 average reward of: -386.64877115660386in 1000 steps\n",
      "Episode: 9/200 return of -433.1538151185781 average reward of: -391.2992755528013in 1000 steps\n",
      "Episode: 10/200 return of -580.2035011587872 average reward of: -394.91158423167803in 1000 steps\n",
      "Episode: 11/200 return of -549.1674703211745 average reward of: -411.0079673462211in 1000 steps\n",
      "Episode: 12/200 return of -348.11931897384613 average reward of: -404.8875680178597in 1000 steps\n",
      "Episode: 13/200 return of -337.51010066667874 average reward of: -397.1411850809497in 1000 steps\n",
      "Episode: 14/200 return of -457.54561645591497 average reward of: -418.08931900478956in 1000 steps\n",
      "Episode: 15/200 return of -531.3105150656464 average reward of: -444.01342242074486in 1000 steps\n",
      "Episode: 16/200 return of -427.18507638314867 average reward of: -456.1739842872231in 1000 steps\n",
      "Episode: 17/200 return of -337.90096925639335 average reward of: -444.19313287608827in 1000 steps\n",
      "Episode: 18/200 return of -358.7607732376317 average reward of: -436.08571566378in 1000 steps\n",
      "Episode: 19/200 return of -434.48770791397504 average reward of: -436.2191049433197in 1000 steps\n",
      "Episode: 20/200 return of -248.2609870390445 average reward of: -403.02485353134546in 1000 steps\n",
      "Episode: 21/200 return of -508.2232825755498 average reward of: -398.930434756783in 1000 steps\n",
      "Episode: 22/200 return of -551.5566990129402 average reward of: -419.27417276069235in 1000 steps\n",
      "Episode: 23/200 return of -477.10054993529405 average reward of: -433.23321768755386in 1000 steps\n",
      "Episode: 24/200 return of -420.3445627290053 average reward of: -429.51311231486295in 1000 steps\n",
      "Episode: 25/200 return of -360.32126442882037 average reward of: -412.4141872511803in 1000 steps\n",
      "Episode: 26/200 return of -516.9908381706286 average reward of: -421.3947634299284in 1000 steps\n",
      "Episode: 27/200 return of -480.90891298183277 average reward of: -435.6955578024722in 1000 steps\n",
      "Episode: 28/200 return of -406.34661008467356 average reward of: -440.45414148717646in 1000 steps\n",
      "Episode: 29/200 return of -266.33519989032203 average reward of: -423.63889068481114in 1000 steps\n",
      "Episode: 30/200 return of -377.11321086952125 average reward of: -436.5241130678588in 1000 steps\n",
      "Episode: 31/200 return of -393.1197232580434 average reward of: -425.01375713610815in 1000 steps\n",
      "Episode: 32/200 return of -517.0128039451415 average reward of: -421.55936762932834in 1000 steps\n",
      "Episode: 33/200 return of -430.86786032115475 average reward of: -416.9360986679144in 1000 steps\n",
      "Episode: 34/200 return of -577.1639733313764 average reward of: -432.6180397281514in 1000 steps\n",
      "Episode: 35/200 return of -397.4775414772051 average reward of: -436.33366743298996in 1000 steps\n",
      "Episode: 36/200 return of -196.74629734447177 average reward of: -404.30921335037425in 1000 steps\n",
      "Episode: 37/200 return of -371.6555376603399 average reward of: -393.38387581822496in 1000 steps\n",
      "Episode: 38/200 return of -526.1797053462958 average reward of: -405.3671853443872in 1000 steps\n",
      "Episode: 39/200 return of -469.0584878538158 average reward of: -425.63951414073654in 1000 steps\n",
      "Episode: 40/200 return of -445.9982304451512 average reward of: -432.5280160982996in 1000 steps\n",
      "Episode: 41/200 return of -295.33854161438444 average reward of: -422.7498979339337in 1000 steps\n",
      "Episode: 42/200 return of -391.92623119374116 average reward of: -410.2412406587936in 1000 steps\n",
      "Episode: 43/200 return of -491.3074548470097 average reward of: -416.2852001113791in 1000 steps\n",
      "Episode: 44/200 return of -468.2522189360014 average reward of: -405.39402467184163in 1000 steps\n",
      "Episode: 45/200 return of -463.3028478596943 average reward of: -411.9765553100906in 1000 steps\n",
      "Episode: 46/200 return of -439.665173105674 average reward of: -436.2684428862108in 1000 steps\n",
      "Episode: 47/200 return of -544.9591449164944 average reward of: -453.5988036118262in 1000 steps\n",
      "Episode: 48/200 return of -467.3561973634279 average reward of: -447.7164528135395in 1000 steps\n",
      "Episode: 49/200 return of -459.7181754867812 average reward of: -446.782421576836in 1000 steps\n",
      "Episode: 50/200 return of -409.4465921580593 average reward of: -443.1272577481268in 1000 steps\n",
      "Episode: 51/200 return of -514.4709229246721 average reward of: -465.0404958791555in 1000 steps\n",
      "Episode: 52/200 return of -556.4305278690484 average reward of: -481.49092554668624in 1000 steps\n",
      "Episode: 53/200 return of -433.86329637484283 average reward of: -475.7465096994696in 1000 steps\n",
      "Episode: 54/200 return of -490.36844714324985 average reward of: -477.95813252019445in 1000 steps\n",
      "Episode: 55/200 return of -480.17482884445104 average reward of: -479.64533061867013in 1000 steps\n",
      "Episode: 56/200 return of -462.63261702336393 average reward of: -481.9420750104391in 1000 steps\n",
      "Episode: 57/200 return of -434.2978903936291 average reward of: -470.87594955815257in 1000 steps\n",
      "Episode: 58/200 return of -454.2149204563468 average reward of: -469.56182186744445in 1000 steps\n",
      "Episode: 59/200 return of -422.27306713858866 average reward of: -465.8173110326252in 1000 steps\n",
      "Episode: 60/200 return of -236.82520620686287 average reward of: -448.5551724375056in 1000 steps\n",
      "Episode: 61/200 return of -469.3643471201573 average reward of: -444.044514857054in 1000 steps\n",
      "Episode: 62/200 return of -361.27268425054814 average reward of: -424.52873049520406in 1000 steps\n",
      "Episode: 63/200 return of -457.42222491440674 average reward of: -426.8846233491604in 1000 steps\n",
      "Episode: 64/200 return of -383.6423177435479 average reward of: -416.2120104091902in 1000 steps\n",
      "Episode: 65/200 return of -403.88381591890146 average reward of: -408.5829091166353in 1000 steps\n",
      "Episode: 66/200 return of -466.6635447127666 average reward of: -408.98600188557555in 1000 steps\n",
      "Episode: 67/200 return of -435.32160783265726 average reward of: -409.0883736294784in 1000 steps\n",
      "Episode: 68/200 return of -484.5319388894957 average reward of: -412.12007547279325in 1000 steps\n",
      "Episode: 69/200 return of -386.6369519788488 average reward of: -408.55646395681924in 1000 steps\n",
      "Episode: 70/200 return of -445.3925124851701 average reward of: -429.41319458464994in 1000 steps\n",
      "Episode: 71/200 return of -290.8199378405892 average reward of: -411.55875365669317in 1000 steps\n",
      "Episode: 72/200 return of -440.50793398610057 average reward of: -419.4822786302485in 1000 steps\n",
      "Episode: 73/200 return of -302.84907003856824 average reward of: -404.02496314266455in 1000 steps\n",
      "Episode: 74/200 return of -445.0384021626236 average reward of: -410.1645715845722in 1000 steps\n",
      "Episode: 75/200 return of -502.2114108368017 average reward of: -419.99733107636223in 1000 steps\n",
      "Episode: 76/200 return of -487.19374251280937 average reward of: -422.05035085636644in 1000 steps\n",
      "Episode: 77/200 return of -462.00022661585155 average reward of: -424.71821273468584in 1000 steps\n",
      "Episode: 78/200 return of -533.6929134553658 average reward of: -429.6343101912729in 1000 steps\n",
      "Episode: 79/200 return of -586.4242438765132 average reward of: -449.61303938103936in 1000 steps\n",
      "Episode: 80/200 return of -340.3504679679226 average reward of: -439.10883492931464in 1000 steps\n",
      "Episode: 81/200 return of -471.5676758423474 average reward of: -457.18360872949035in 1000 steps\n",
      "Episode: 82/200 return of -485.0134212637472 average reward of: -461.63415745725507in 1000 steps\n",
      "Episode: 83/200 return of -432.6862576824557 average reward of: -474.6178762216438in 1000 steps\n",
      "Episode: 84/200 return of -344.9832578777885 average reward of: -464.6123617931604in 1000 steps\n",
      "Episode: 85/200 return of -486.6034654699837 average reward of: -463.0515672564784in 1000 steps\n",
      "Episode: 86/200 return of -462.79666976448925 average reward of: -460.61185998164655in 1000 steps\n",
      "Episode: 87/200 return of -506.66947009805216 average reward of: -465.0787843298666in 1000 steps\n",
      "Episode: 88/200 return of -435.2857412323417 average reward of: -455.2380671075642in 1000 steps\n",
      "Episode: 89/200 return of -381.1249151963399 average reward of: -434.7081342395469in 1000 steps\n",
      "Episode: 90/200 return of -445.3833545576673 average reward of: -445.21142289852133in 1000 steps\n",
      "Episode: 91/200 return of -412.9724897410529 average reward of: -439.3519042883919in 1000 steps\n",
      "Episode: 92/200 return of -363.5743860622695 average reward of: -427.20800076824406in 1000 steps\n",
      "Episode: 93/200 return of -481.2753373823305 average reward of: -432.0669087382315in 1000 steps\n",
      "Episode: 94/200 return of -380.9626695375034 average reward of: -435.66484990420304in 1000 steps\n",
      "Episode: 95/200 return of -455.3042568837599 average reward of: -432.5349290455807in 1000 steps\n",
      "Episode: 96/200 return of -512.0739253920796 average reward of: -437.4626546083397in 1000 steps\n"
     ]
    }
   ],
   "source": [
    "for episode in range(episodes):\n",
    "\n",
    "  Return = 0\n",
    "\n",
    "  done = False\n",
    "  state = env.reset()\n",
    "  steps = 0\n",
    "\n",
    "  while not done:\n",
    "\n",
    "    action = sac.policy.get_action(state)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    Return += reward\n",
    "    experience_replay.append(state, next_state, action, reward, done, info)\n",
    "    state = next_state\n",
    "    steps += 1\n",
    "\n",
    "  sac.train()\n",
    "\n",
    "  returns.append(Return)\n",
    "  avg_return = np.mean(returns[-10:])\n",
    "  avg_returns.append(avg_return)\n",
    "\n",
    "  total_steps += steps\n",
    "\n",
    "  print(\"Episode: \" + str(episode)+\"/\"+str(episodes) + \" return of \"+ str(Return) + \" average reward of: \" + str(avg_return) +  \"in \" + str(steps) + \" steps\")\n",
    "\n",
    "  if episode % 10 == 0:\n",
    "    sac.dqn.save_weights(f\"critic1_{ episode }\", save_format='tf')\n",
    "    sac.dqn2.save_weights(f\"critic2_{ episode }\", save_format='tf')\n",
    "    \n",
    "    sac.value_net.save_weights(f\"value_{ episode }\", save_format='tf')\n",
    "\n",
    "    sac.policy.save_weights(f\"actor_{ episode }\", save_format='tf')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
